---
title: ""
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# R Open Labs
## Models
#### University of North Carolina at Chapel Hill
#### Matt Jansen
#### November 8, 2018

## Data and other downloads

```{r message=FALSE, warning=FALSE}
library(tidyverse)
#install.packages("gridExtra")
library(gridExtra) #combining and organizing multiple ggplots
#install.packages("caret")
library(caret) #wrapper for fitting and comparing models
#install.packages("stargazer")
library(stargazer) #formatted journal-friendly reports for linear models
```

## Today

This workshop aims to introduce some **basic** intuition about modelling in R.

* [R for Data Science, Chapters 22-25](https://r4ds.had.co.nz/model-intro.html)

## Reminder: R Markdown

If you'd like more practice with R Markdown, consider keeping your work from this week in an R Markdown document.  If you need a refresher, refer to [last week's materials](https://unc-libraries-data.github.io/R-Open-Labs/week7_RMarkdown/R_OpenLabs_7_Rmarkdown.html).

## Terminology

### What is a model?

A model is another tool to summarize the information in a dataset.  This often comes in the form of approximations that make it easier to interpret and generalize relationships to new data.  Ideally a model represents the "signal" in the data while ignoring unimportant or ungeneralizable "noise."

For today we'll mainly consider "supervised models".  A supervised model aims to predict some target variable(s) and requires that we have observed these variables alongside the factors we want to use to predict it.  This target variable is sometimes called a "dependent" or "response" variable.

Unsupervised models don't distinguish between response and predictor variables and instead look for patterns among variables more generally. Clustering is one of the most commonly used unsupervised learning methods.

For simplicity, we'll think about our models in terms of prediction.


## Simple Linear Models with Plots

For these plots, we'll use `moddf` provided via the code below.  The `url` function combined with `read_csv` allows us to directly import content from the internet into our R session.

We'll look at the code to generate `moddf` at the end of this section.

```{r message=FALSE}
moddf <- read_csv(url("https://github.com/UNC-Libraries-data/R-Open-Labs/raw/master/week8_Models/moddf.csv"))
head(moddf)
```

Since these are both continuous numeric variables, let's plot them with a scatterplot.

```{r fig.width=4, fig.height=4}
ggplot(data=moddf,aes(x=x,y=y))+geom_point()+theme_bw()
```

One of the simplest ways to model data like this is with a line capturing the trend.  We can model a line with an interpretable formula: 

$$y= intercept + slope * x$$

Given a new `x` value, we could predict the corresponding `y` by plugging it into this equation.  We can also get observations for the `x` values we have observed.

We can plot some candidate line(s) with `geom_abline`:

```{r fig.width=4, fig.height=4}
ggplot(data=moddf,aes(x=x,y=y))+geom_point()+
  geom_abline(intercept=-1,slope=3.6,color="blue")+
  geom_abline(intercept=2,slope=3,color="orange")+
  theme_bw()
```

How do we choose between lines?  A common metric, especially in prediction, involves measuring our misses with residuals:

residual = actual value - prediction

These can be graphically represented as lines parallel to the y-axis:

```{r fig.width=8, fig.height=4,message=FALSE, warning=FALSE}
moddf$p1 <- -1 + 3.6 * moddf$x
moddf$p2 <- 2 + 3 * moddf$x
plot1 <- ggplot(data=moddf,aes(x=x,y=y))+geom_point()+
  geom_abline(intercept=2,slope=3,color="orange")+
  geom_segment(aes(x = x, y = y,
                   xend = x, yend = p2),color="orange")+
  theme_bw()

plot2 <- ggplot(data=moddf,aes(x=x,y=y))+geom_point()+
  geom_abline(intercept=-1,slope=3.6,color="blue")+
  geom_segment(aes(x = x, y = y,
                   xend = x, yend = p1),color="blue")+
  theme_bw()

grid.arrange(plot1,plot2,ncol=2)
```

Ordinary Least Squares (OLS), often referred to simply as "Linear Regression", finds the line that minimizes the sum of the squared residuals above.

We can fit an OLS with `lm` in R, and get a standard statistical summary with `summary`.

```{r}
fit <- lm(y ~ x, data = moddf)
summary(fit)
```

The arguments given to `lm` are:

* Formula: `y~x`
    + R uses `~` in place of the equals sign in formulas
    + R automatically includes an intercept term
    + This formula is therefore equivalent to: 
    
      $$y=intercept + slope * x$$
* Data: 
    + The dataframe containing the variables of interest.
    
The values listed under `Estimate` provide estimates for the intercept,`r fit$coefficient[1]`, and slope,`r fit$coefficient[2]`. 

```{r fig.width=4, fig.height=4}
moddf$lm <- predict(fit)
ggplot(data=moddf, aes(x=x, y=y)) + 
  geom_point(color="black") + 
  geom_abline(intercept=fit$coefficients[1],slope=fit$coefficients[2],size=1.1,alpha=0.8,color="black") +
  geom_abline(intercept=2,slope=3,color="orange")+
  geom_abline(intercept=-1,slope=3.6,color="blue")+
  geom_segment(aes(x = x, y = y,
                   xend = x, yend = lm)) +
  theme_bw()
```

Finally, here's the code used to generate our dataset:

```{r}
x <- runif(50,0,15)
y <- 3.4 * x + rnorm(50,0,5)
moddf <- data.frame(x,y)
```

The "true" values, obscured by random noise, were:

* intercept = 0
* slope = 3.4

The OLS did a pretty good job determining these numbers.

## Multiple Regression - Formula notation in R

OLS can be extended to include multiple predictors.  Let's experiment with the built-in `mtcars` dataset.

```{r}
data(mtcars)
str(mtcars)
```

We add more variables with extentions to R's formula notation:

|Symbol|Role|Example|Equivalent|
|----|-----------|-----------|-----------------|
|`+`|Add variable|`mpg~vs+disp`|$$mpg = intercept + \beta_1 vs + \beta_2 disp$$|
|`*`|Interactions|`mpg~vs*disp`|$$mpg = intercept + \beta_1 vs + \beta_2 disp + \beta_3 vs*disp$$|
|`.`|Include all variables in dataframe| `mpg~.` | $$mpg = intercept + \beta_1 cyl + \beta_2 disp + ... + \beta_{10} carb$$|
|`-`|Exclude variable|`mpg~.-disp-hp`|$$mpg = intercept + \beta_1 cyl + \beta_2 drat + ... + \beta_{8} carb$$|

Examples:

```{r}
summary(lm(data=mtcars,mpg~vs+disp))
```

What does `-1` do below?

```{r}
summary(lm(data=mtcars,mpg~vs*hp+disp-1))
```

## `caret` Classification and Regression Training

The `caret` package provides a uniform interface for fitting 237 different models.

We'll use the built-in dataset `Sacramento`, containing data on 932 home sales in Sacramento, CA over a five-day period.

```{r}
data(Sacramento)
str(Sacramento)
```

First we'll split our dataset into two parts:

* A `training` dataset we'll use to **fit** our models.
* A `testing` dataset we'll set aside for comparison after fitting the models.  This helps avoid *overfitting* the `training` set by examining how it fits unseen data.

```{r}
set.seed(12345)
train.select <- createDataPartition(Sacramento$type, p = .8, list = FALSE)
training <- Sacramento[ train.select,]
testing  <- Sacramento[-train.select,]
```

Many of the more complicated models we can fit with `caret` need to determine optimal settings for various "tuning" parameters.

We can use Repeated k-fold Cross Validation (among other methods) to determine the best values for the tuning parameters within default ranges.  In practice you may want to supply your own grid of possible tuning parameter values.  [Read more here](http://topepo.github.io/caret/model-training-and-tuning.html#basic-parameter-tuning).

```{r}
fitControl <- trainControl(## 5-fold Cross Validation
                           method = "repeatedcv",
                           number = 5,
                           ## repeated ten times
                           repeats = 10)
```

The `train` function is used to fit models.  The [full list of models is available here](http://topepo.github.io/caret/available-models.html).  You can get more informtation about a model and its tuning parameters with `getModelInfo(<model name>)`.

We'll fit several example models all attempting to predict home price from all of the other variables except zip code and city (these have many unique values and complicate the models).

```{r results="hide", warning=FALSE,message=FALSE}
#Ordinary Least Squares
set.seed(8947) # ensures paired resampling later
lmfit <- train(price ~ .-zip-city, data = training, 
                 method = "lm",
                 trControl = fitControl)

#Robust Linear Model
set.seed(8947) # ensures paired resampling later
robustfit <- train(price ~ .-zip-city, data = training, 
                 method = "rlm", 
                 trControl = fitControl,
                 verbose = FALSE)

#Random Forests
set.seed(8947) # ensures paired resampling later
rffit <- train(price ~ .-zip-city, data = training, 
                 method = "ranger", 
                 trControl = fitControl,
                 verbose = FALSE)


#XGBoost (a refinement of Random Forests) - this is the slowest model!
set.seed(8947) # ensures paired resampling later
xgbfit <- train(price ~ .-zip-city, data = training,
                 method = "xgbTree",
                 trControl = fitControl,
                 verbose = FALSE)


#Support Vector Machine with Linear Kernel
set.seed(8947) # ensures paired resampling later
svmfit <- train(price ~ .-zip-city, data = training, 
                 method = "svmLinear", 
                 trControl = fitControl,
                 verbose = FALSE)
```

**Notes**

* `caret` is just a wrapper for fitting models - it does not include functions to fit many of these models.  When fitting a model you may see the following message:
```
1 package is needed for this model and is not installed. (<package-name>). Would you like to try to install it now?
1: yes
2: no
```
Press 1 and hit enter to install the package and fit the model.  Packages needed are listed in the [Available models list](http://topepo.github.io/caret/available-models.html).

We've intentionally made sure a few things are consistent across our models to make comparisons easier:

* Each model has the same response variable, `price`.

To ensure we can compare between models with [*resampling*](https://en.wikipedia.org/wiki/Resampling_(statistics)):

* Each model is fit using the same `trControl=fitControl` setting.
* Each model has the seed set in the same way before each `train` call.  
    + I've used `set.seed(8947)`; the number `8947` is unimportant, it just needs to be consistent.


### Resampling

The `resamples` function considers the models against datasets simulated by sampling from the training set with replacement.  You may be familiar with the related concept of "bootstrapping".

`caret` gives us three different indices to compare these models:

* Mean Absolute Error (MAE)
* Root Mean Squared Error (RMSE)
* R Squared ([See note on calculation here](https://topepo.github.io/caret/measuring-performance.html#reg))

These track how well the model fits the data in different ways.  Without getting into the details of how they're calculated, we'll  use the rules of thumb that:

* **Lower** Errors (MAE and RMSE) are better
* **Higher** R-squared values are better

Note: `caret` provides different metrics (Kappa and accuracy) for classification (i.e. categorical outcomes) tasks.

```{r}
results <- resamples(list("OLS"=lmfit,"Random.Forest"=rffit,
                          "Robust.LM"=robustfit,"SVM"=svmfit,
                          "xgbTree"=xgbfit))
summary(results)
```

We can also present these results in graphical form:

```{r, fig.height=4, fig.width=12}
bwplot(results,scales=list(relation="free"))
```

Remember that these results are across a number of resamples, hence the boxplots and not a single value per model!

The random forest and xgbTree seem to be doing well here, but it's not clear that one is clearly outperforming the other.

### Out of Sample Performance

Let's revisit our `testing` data.  

We can use our models to generate predictions with `predict`, then compare their performance with `postResample`.

```{r}
lm.test <- predict(lmfit,testing)
robust.test <- predict(robustfit,testing)
rf.test <- predict(rffit,testing)
xgb.test <- predict(xgbfit,testing)
svm.test <- predict(svmfit,testing)
train.results <- rbind(
  "LM"=postResample(pred=lm.test,obs=testing$price),
  "Robust"=postResample(pred=robust.test,obs=testing$price),
  "Random Forest"=postResample(pred=rf.test,obs=testing$price),
  "SVM"=postResample(pred=svm.test,obs=testing$price),
  "xgbTree"=postResample(pred=xgb.test,obs=testing$price)
)
print(train.results)
```

Which model seems to do best?

## Learn more

###  `caret`

* [Documentation](http://topepo.github.io/caret/index.html)
* [Applied Predictive Modeling](https://search.lib.unc.edu/search?R=UNCb7414199)

### Modeling

You can learn more about common extensions to linear models in R For Data Science:

* [Categorical Variables, 23.4.1](https://r4ds.had.co.nz/model-basics.html#categorical-variables)
* Interactions, [23.4.2](https://r4ds.had.co.nz/model-basics.html#interactions-continuous-and-categorical)-[23.4.3](https://r4ds.had.co.nz/model-basics.html#interactions-two-continuous)
* [Transformations, 23.4.4](https://r4ds.had.co.nz/model-basics.html#transformations)

Linear models alone are often the foundation for an entire semester long course; taught in a variety of discipline-specific settings as well as the Biostatistics and Statistics and Operations Research departments.

## Extra: Formatting `lm` style model results - `stargazer` package

Once you've fit a linear or some other model, you may want to report results. The `stargazer` package makes this relatively simple to do, especially in an R Markdown document.  First let's fit a simple model with built-in data:

```{r}
data(iris)
mod <- lm(data=iris,Sepal.Length~Petal.Length+Petal.Width+Species)
summary(mod)
```

The below code will produce a common model summary format for a journal or presentation.  

The code block has the R markdown option `{r results = "asis"}`, which instructs R Markdown to use the HTML code that `stargazer` produces as part of the output document.


```{r results = "asis", message=FALSE, warning=FALSE} 
stargazer(mod, type = "html",  #we use html output to match our planned R Markdown output
     title = "My iris model")
```

We can also write the table directly to a file with the `out` argument:

```{r  results = 'hide'}
stargazer(mod, type = "html", out = "regression.html" ,title = "My iris model")
```

There's a useful cheatsheet for `stargazer` and its myriad customization options [here.](https://www.jakeruss.com/cheatsheets/stargazer/)

## Exercises

1. Explore the datasets included with the `caret` package by running `data(package="caret")`.  You can get more information about any given dataset with `?<dataset name>`.  Choose a dataset with at least a few numeric variables.

2. Use `str`, `head`, `summary` and any of our EDA techniques to explore the dataset.  Pick a continuous outcome variable of interest and choose some predictors.

3. Fit an `lm` with your selected variables.

4. Use `caret` to compare your `lm` to another model of your choice (choose something else with "Regression" listed as its type [here](https://topepo.github.io/caret/available-models.html)).

5. Use your intial `lm` fit to create a stargazer table output.  Embed in an R Markdown document or output html.  Use `?stargazer` to learn about and then change one or more default settings.


## Feedback
[Let us know what you think of this lesson!](https://unc.az1.qualtrics.com/jfe/form/SV_8e1zRY2rlFUYBMx)